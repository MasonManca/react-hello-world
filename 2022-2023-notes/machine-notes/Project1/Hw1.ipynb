{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align= 'center'>Project: The Dumb Predictor</h1>\n",
    "<h5>Mason Manca </h5>\n",
    "<h5>CPSC 323 01 </h5>\n",
    "<h5>Dr.Morehead </h5>\n",
    "<h5>8 September 2022 </h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell \n",
    "import time\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell holds all methods    \n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,df):\n",
    "        predictionList = []\n",
    "        for row in enumerate(df.iterrows()):\n",
    "            predictionList.append(1)       \n",
    "    \n",
    "        return(predictionList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      " Shortened to 50 for conciseness\n"
     ]
    }
   ],
   "source": [
    "# Cell for output of data and time calculation\n",
    "model = Model()\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "t0 = time.time() # Start of the clock to determine performance\n",
    "\n",
    "predictions = model.predict(df)\n",
    "\n",
    "timeOfModel = time.time() - t0 # Calculation of performance speed\n",
    "\n",
    "# Prediction output: \n",
    "# Always going to be false because it is the \"Dumb Predictor\"\n",
    "print(predictions[:50], \"\\n Shortened to 50 for conciseness\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Accuracy, F1 Score and Confusion Matrix\n",
    "\n",
    "Since we can only have 'false positives' and 'true positives' since we are predicting a boolean value with a model that only returns 'true'. We are able to determine accuracy simply by counting the number of correct identifications (TP) and dividing by all inputs.\n",
    "\n",
    "The rest of the values are False positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.55\n",
      "F1 Score:  0.71\n",
      "Speed of Model:  0.04\n"
     ]
    }
   ],
   "source": [
    "# This cell is for Analysis of accuracy, F1 score \n",
    "# Creating variables for analysis\n",
    "\n",
    "\n",
    "TP = int(df['HeartDisease'].value_counts()[1]) # Predicted positive, are positive\n",
    "FP = int(df['HeartDisease'].value_counts()[0]) # Predicted positive are negative\n",
    "FN = 0 # Predicted negative are positive\n",
    "TN = 0 # Predicted negative are negative\n",
    "\n",
    "allVals = 918\n",
    "\n",
    "# Accuracy \n",
    "accuracy = TP/allVals # number of inputs\n",
    "print('Accuracy: ', round(accuracy,2))\n",
    "\n",
    "\n",
    "# F1 score: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "precision = TP/(TP + FP) # in this case because of the model TP/(TP+FP)\n",
    "recall = TP/(TP + FN)\n",
    "\n",
    "F1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "print(\"F1 Score: \", round(F1_score,2))\n",
    "\n",
    "print(\"Speed of Model: \", round(timeOfModel,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix\n",
    "\n",
    "|       | Predicted (0)  | Predicted (918)|\n",
    "| ----------- | ----------- |----------- |\n",
    "|Actual (410)| TN    (0)   | FP (410)    |\n",
    "|Actual (508)| FN    (0)   | TP  (508)   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Analysis:\n",
    "- 55% accurate over 918 cases\n",
    "\n",
    "F1 Score Analysis:\n",
    "- Harmonic mean of .71\n",
    "\n",
    "Confusion Matrix Analysis:\n",
    "- 410 False Positives\n",
    "- 508 True Positives\n",
    "\n",
    "Performance Speed:\n",
    "- Approx. 0.03 sec over 918 instances\n",
    "\n",
    "<h4>Conclusion: A dumb predictor</h4>\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efc2cc036f59d410b7b7a9272940ad63ce6169c26158ebd6d1ae777fa26105c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
